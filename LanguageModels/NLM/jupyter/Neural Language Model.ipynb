{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import functools\n",
    "import requests\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/jcjohnson/torch-rnn/master/data/tiny-shakespeare.txt'\n",
    "text = requests.get(url).content.decode('utf-8')\n",
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_tokens(sent, tokenizer):\n",
    "    words = tokenizer.tokenize(sent)\n",
    "    tokens = []\n",
    "    for w in words:\n",
    "        w = re.sub(r'https?://\\S+', '<URL>', w)\n",
    "        w = re.sub(r'#\\S+', '<TOPIC>', w)\n",
    "        w = re.sub(r'@\\S+', '<USER>', w)\n",
    "        tokens.append(w)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def basic_text_preprocess(text):\n",
    "    # lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    text = text.lower().replace('-\\n', '')\n",
    "    custom_tokenize = functools.partial(sent_to_tokens, tokenizer=nltk.TweetTokenizer())\n",
    "    tokens = [*map(custom_tokenize, nltk.tokenize.sent_tokenize(text))]\n",
    "    # tokens = [[lemmatizer.lemmatize(word) for word in sent] for sent in tokens]\n",
    "    print(f'corpus size: {len(tokens)}')\n",
    "    return tokens\n",
    "\n",
    "tokens = basic_text_preprocess(text)\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(tokens):\n",
    "    tok2idx = collections.defaultdict(lambda: 0)\n",
    "    unique_tokens = list(functools.reduce(lambda a, b: set(a).union(b), tokens))\n",
    "    print(len(unique_tokens))\n",
    "    if len(unique_tokens) > 1000:\n",
    "        max_len = int(len(unique_tokens) * 0.8)\n",
    "        wc = collections.Counter(functools.reduce(lambda a, b: a + b, tokens))\n",
    "        unique_tokens = [*map(lambda w: w[0], wc.most_common(max_len))]\n",
    "    print(f'vocab size: {len(unique_tokens)}')\n",
    "    idx2tok = ['<UNK>', '<START>', '<END>', '<PAD>'] + unique_tokens\n",
    "    for i, tok in enumerate(idx2tok):\n",
    "        tok2idx[tok] = i\n",
    "    return tok2idx, idx2tok\n",
    "\n",
    "tok2idx, idx2tok = build_vocab(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_train(x, tok2idx, seq_len):\n",
    "    x = [tok2idx['<START>']] + x\n",
    "    if len(x) >= seq_len:\n",
    "        return np.array(x[:seq_len], dtype=np.int32), seq_len\n",
    "    pad_x = x + [tok2idx['<PAD>']] * (seq_len - len(x))\n",
    "    return np.array(pad_x, dtype=np.int32), len(x)\n",
    "\n",
    "\n",
    "def pad_target(y, tok2idx, seq_len):\n",
    "    y = y + [tok2idx['<END>']]\n",
    "    if len(y) >= seq_len:\n",
    "        return np.array(y[:seq_len], dtype=np.int32), seq_len\n",
    "    pad_y = y + [tok2idx['<PAD>']] * (seq_len - len(y))\n",
    "    return np.array(pad_y, dtype=np.int32), len(y)\n",
    "\n",
    "\n",
    "def batches_generator(batch_size, tokens, tok2idx, seq_len):\n",
    "    n_samples = len(tokens)\n",
    "    order = np.random.permutation(n_samples) # shuffle data\n",
    "    n_batches = n_samples // batch_size + 1\n",
    "    for k in range(n_batches):\n",
    "        batch_start = k * batch_size\n",
    "        batch_end = min((k + 1) * batch_size, n_samples)\n",
    "        current_batch_size = batch_end - batch_start\n",
    "        x_list, y_list, max_len = [], [], 0\n",
    "        for idx in order[batch_start:batch_end]:\n",
    "            x_list.append([tok2idx['<START>']]+[tok2idx[t] for t in tokens[idx][:-1]])\n",
    "            y_list.append([tok2idx[t] for t in tokens[idx][1:]] + [tok2idx['<END>']])\n",
    "            max_len = max(max_len, len(tokens[idx]))\n",
    "        X = np.ones([current_batch_size, max_len], dtype=np.int32) * tok2idx['<PAD>']\n",
    "        Y = np.ones([current_batch_size, max_len], dtype=np.int32) * tok2idx['<PAD>']\n",
    "        actual_lengths = np.zeros(current_batch_size, dtype=np.int32)\n",
    "        for n in range(current_batch_size):\n",
    "            tok_len = len(x_list[n])\n",
    "            actual_lengths[n] = tok_len\n",
    "            X[n, :tok_len] = x_list[n]\n",
    "            Y[n, :tok_len] = y_list[n]\n",
    "        yield X, Y, actual_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralLanguageModel:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def declare_placeholders(self):\n",
    "    self.input_batch = tf.placeholder(dtype=tf.int32, shape=[None, None], name='input_batch')\n",
    "    self.target_batch = tf.placeholder(dtype=tf.int32, shape=[None, None], name='target_batch')\n",
    "    self.lengths = tf.placeholder(dtype=tf.int32, shape=[None], name='lengths')\n",
    "    self.dropout_ph = tf.placeholder_with_default(tf.cast(1.0, tf.float32), shape=[], name='dropout_rate')\n",
    "    self.learning_rate_ph = tf.placeholder(dtype=tf.float32, shape=[], name='learning_rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NeuralLanguageModel.__declare_placeholders = classmethod(declare_placeholders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_layers(self, vocabulary_size, embedding_dim, n_hidden):\n",
    "    initial_embedding_matrix = np.random.randn(vocabulary_size, embedding_dim) / np.sqrt(embedding_dim)\n",
    "    embedding_matrix = tf.Variable(initial_embedding_matrix, dtype=tf.float32, name='embedding_matrix')\n",
    "    \n",
    "    rnn_cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)\n",
    "    # rnn_cell = tf.nn.rnn_cell.BasicRNNCell(num_units=n_hidden, activation=tf.nn.tanh)\n",
    "    regulrized_rnn_cell = tf.nn.rnn_cell.DropoutWrapper(\n",
    "        cell=rnn_cell, input_keep_prob=self.dropout_ph, output_keep_prob=self.dropout_ph,\n",
    "        state_keep_prob=self.dropout_ph)\n",
    "    \n",
    "    mul_cell = tf.nn.rnn_cell.MultiRNNCell(cells=[regulrized_rnn_cell]*2)\n",
    "    embeddings = tf.nn.embedding_lookup(embedding_matrix, self.input_batch)\n",
    "    output, state = tf.nn.dynamic_rnn(cell=mul_cell,\n",
    "                                      inputs=embeddings,\n",
    "                                      sequence_length=self.lengths,\n",
    "                                      dtype=tf.float32)\n",
    "    self.logits = tf.layers.dense(output, vocabulary_size, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NeuralLanguageModel.__build_layers = classmethod(build_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_predictions(self):\n",
    "    softmax_output = tf.nn.softmax(self.logits)\n",
    "    self.predictions = tf.argmax(softmax_output, axis=-1)\n",
    "    self.probs = softmax_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NeuralLanguageModel.__compute_predictions = classmethod(compute_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(self, vocabulary_size, pad_index):\n",
    "    targets_one_hot = tf.one_hot(self.target_batch, vocabulary_size)\n",
    "    loss_tensor = tf.nn.softmax_cross_entropy_with_logits_v2(labels=targets_one_hot,\n",
    "                                                             logits=self.logits)\n",
    "    mask = tf.cast(tf.not_equal(self.input_batch, pad_index), tf.float32)\n",
    "    self.loss = tf.reduce_mean(tf.boolean_mask(loss_tensor, mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NeuralLanguageModel.__compute_loss = classmethod(compute_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_optimization(self):\n",
    "    self.optimizer = tf.train.AdamOptimizer(self.learning_rate_ph)\n",
    "    grads_and_vars = self.optimizer.compute_gradients(self.loss)\n",
    "    \n",
    "    clip_norm = tf.cast(5.0, tf.float32)\n",
    "    self.grads_and_vars = [(tf.clip_by_norm(grad, clip_norm), var) for grad, var in grads_and_vars]\n",
    "    \n",
    "    self.train_op = self.optimizer.apply_gradients(self.grads_and_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NeuralLanguageModel.__perform_optimization = classmethod(perform_optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(self, vocabulary_size, embedding_dim, n_hidden, pad_index):\n",
    "    self.__declare_placeholders()\n",
    "    self.__build_layers(vocabulary_size, embedding_dim, n_hidden)\n",
    "    self.__compute_predictions()\n",
    "    self.__compute_loss(vocabulary_size, pad_index)\n",
    "    self.__perform_optimization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NeuralLanguageModel.__init__ = classmethod(init_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_batch(self, sess, x_batch, y_batch, lengths, learning_rate, dropout_keep_prob):\n",
    "    feed_dict = {\n",
    "        self.input_batch: x_batch,\n",
    "        self.target_batch: y_batch,\n",
    "        self.learning_rate_ph: learning_rate,\n",
    "        self.dropout_ph: dropout_keep_prob,\n",
    "        self.lengths: lengths\n",
    "    }\n",
    "    sess.run(self.train_op, feed_dict=feed_dict)\n",
    "    return sess.run(self.loss, feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NeuralLanguageModel.train_on_batch = classmethod(train_on_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_for_batch(self, sess, x_batch, lengths):\n",
    "    feed_dict = {\n",
    "        self.input_batch: x_batch,\n",
    "        self.lengths: lengths\n",
    "    }\n",
    "    predictions = sess.run(self.predictions, feed_dict=feed_dict)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NeuralLanguageModel.predict_for_batch = classmethod(predict_for_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "model = NeuralLanguageModel(vocabulary_size=len(tok2idx),\n",
    "                            embedding_dim=256,\n",
    "                            n_hidden=256,\n",
    "                            pad_index=tok2idx['<PAD>'])\n",
    "batch_size = 50\n",
    "n_epochs = 4\n",
    "learning_rate = 0.005\n",
    "learning_rate_decay = 1.1\n",
    "dropout_keep_probability = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for epoch in range(n_epochs):\n",
    "    # For each epoch evaluate the model on train and validation data\n",
    "    print('-' * 20 + ' Epoch {} '.format(epoch+1) + 'of {} '.format(n_epochs) + '-' * 20)\n",
    "    \n",
    "    # Train the model\n",
    "    losses = []\n",
    "    for x_batch, y_batch, lengths in batches_generator(batch_size, tokens, tok2idx, 100):\n",
    "        batch_loss = model.train_on_batch(sess, x_batch, y_batch, lengths, learning_rate,\n",
    "                                          dropout_keep_probability)\n",
    "        losses.append(batch_loss * len(x_batch))\n",
    "    print(f'traning loss: {np.sum(losses)/len(tokens)}')\n",
    "        \n",
    "    # Decaying the learning rate\n",
    "    learning_rate = learning_rate / learning_rate_decay\n",
    "    \n",
    "print('...training finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cond_prob(sess, model, seq_toks):\n",
    "    feed_dict = {\n",
    "        model.input_batch: seq_toks,\n",
    "        model.lengths: [len(seq_toks)+10] * len(seq_toks)\n",
    "    }\n",
    "    return np.log1p(sess.run(model.probs, feed_dict=feed_dict)[:, -1])\n",
    "\n",
    "\n",
    "def find_topk_2d(arr, topk):\n",
    "#     arr[:, 2] = 0\n",
    "#     arr[:, 6] = 0\n",
    "#     arr[:, 5] = 0\n",
    "#     arr[:, 7] = 0\n",
    "#     arr[:, 11] = 0\n",
    "#     arr[:, 4] = 0\n",
    "    order = arr.reshape(-1,).argsort()[::-1][:topk]\n",
    "    idx = np.unravel_index(order, arr.shape)\n",
    "    vals = []\n",
    "    for i in range(topk):\n",
    "        vals.append(arr[idx[0][i], idx[1][i]])\n",
    "    return vals, idx\n",
    "\n",
    "\n",
    "def init_beam_search(sess, model, start_toks, topk):\n",
    "    cond_probs = get_cond_prob(sess, model, start_toks)\n",
    "    vals, idx = find_topk_2d(cond_probs, topk)\n",
    "    seq_toks = np.column_stack((np.repeat(start_toks, topk, axis=0),\n",
    "                                idx[1].reshape(-1,1)))\n",
    "    seq_probs = np.reshape(vals, (-1, 1))\n",
    "    return seq_toks, seq_probs\n",
    "\n",
    "\n",
    "def extend_seq(seq_toks, idx):\n",
    "    new_seq = []\n",
    "    for i in range(len(seq_toks)):\n",
    "        seq_idx = idx[0][i]\n",
    "        tok_idx = idx[1][i]\n",
    "        new_seq.append(np.array(list(seq_toks[seq_idx]) + [tok_idx]))\n",
    "    return np.array(new_seq)\n",
    "\n",
    "\n",
    "def calculate_seq_probs(seq_probs, cond_probs):\n",
    "    return cond_probs + seq_probs.reshape(-1, 1)\n",
    "    \n",
    "\n",
    "def itrate_search(sess, seq_toks, seq_probs, topk):\n",
    "    cond_probs = get_cond_prob(sess, model, seq_toks)\n",
    "    seq_probs = calculate_seq_probs(seq_probs, cond_probs)\n",
    "    vals, idx = find_topk_2d(seq_probs, topk)\n",
    "    seq_toks = extend_seq(seq_toks, idx)\n",
    "    seq_probs = seq_probs[idx[0], idx[1]]\n",
    "    return seq_toks, seq_probs\n",
    "\n",
    "def beam_search(sess, model, start_toks, topk, idx2tok):\n",
    "    detokenizer = nltk.tokenize.treebank.TreebankWordDetokenizer()\n",
    "    seq_toks, seq_probs = init_beam_search(sess, model, start_toks, topk)\n",
    "    for i in range(10):\n",
    "        seq_toks, seq_probs = itrate_search(sess, seq_toks, seq_probs, topk)\n",
    "    output_sents = []\n",
    "    for seq in seq_toks:\n",
    "        sent = detokenizer.detokenize([idx2tok[i] for i in seq])\n",
    "        output_sents.append(sent)\n",
    "    return output_sents, seq_toks, seq_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig = tokens[99]\n",
    "detokenizer = nltk.tokenize.treebank.TreebankWordDetokenizer()\n",
    "print(detokenizer.detokenize(orig))\n",
    "start = orig[:1]\n",
    "print(start)\n",
    "start_toks = [[tok2idx['<START>']] + [tok2idx[w] for w in start]]\n",
    "output_sents, seq_toks, seq_probs = beam_search(sess, model, start_toks, 20, idx2tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok2idx[',']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_toks = extend_seq(seq_toks, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.reshape(-1, ).argsort()[::-1][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.unravel_index([7, 3, 5], x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk = 3\n",
    "for i in range(topk):\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
