{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "NLM_staticRNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ir7jnJ3xJt68",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uma0C4nuIwGk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import collections\n",
        "import functools\n",
        "import requests\n",
        "import nltk\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkqB2SnEKjv-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8c4e49bf-dfb8-445e-9b3d-2a5c9a288fc7"
      },
      "source": [
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6YCvLAaIwGq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "67f60658-67d0-4c72-ffce-f2a35794ea51"
      },
      "source": [
        "url = 'https://raw.githubusercontent.com/jcjohnson/torch-rnn/master/data/tiny-shakespeare.txt'\n",
        "text = requests.get(url).content.decode('utf-8')\n",
        "text[:100]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtVoRKJ7JKIS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "3aab29ae-3f51-45ea-fb8b-66febb333b05"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPqWh0UDIwGw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "3ae8aedb-8d66-4904-e415-ce43268ac4e0"
      },
      "source": [
        "def sent_to_tokens(sent, tokenizer):\n",
        "    words = tokenizer.tokenize(sent)\n",
        "    tokens = []\n",
        "    for w in words:\n",
        "        w = re.sub(r'https?://\\S+', '<URL>', w)\n",
        "        w = re.sub(r'#\\S+', '<TOPIC>', w)\n",
        "        w = re.sub(r'@\\S+', '<USER>', w)\n",
        "        tokens.append(w)\n",
        "    return tokens\n",
        "\n",
        "\n",
        "def basic_text_preprocess(text):\n",
        "    # lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "    # regtext = text.lower().replace('-\\n', '')\n",
        "    # custom_tokenize = functools.partial(sent_to_tokens, tokenizer=nltk.TweetTokenizer())\n",
        "    # custom_tokenize = lambda sent: sent.split(' ')\n",
        "    # tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
        "    tokens = [nltk.regexp_tokenize(s, r'[^\\s\\n]+') for s in nltk.tokenize.sent_tokenize(text)]\n",
        "    print(f'corpus size: {len(tokens)}')\n",
        "    return tokens\n",
        "\n",
        "tokens = basic_text_preprocess(text)\n",
        "tokens[:2]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "corpus size: 12460\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['First',\n",
              "  'Citizen:',\n",
              "  'Before',\n",
              "  'we',\n",
              "  'proceed',\n",
              "  'any',\n",
              "  'further,',\n",
              "  'hear',\n",
              "  'me',\n",
              "  'speak.'],\n",
              " ['All:', 'Speak,', 'speak.']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDn_yNreIwG1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8ffdc0a3-e710-43df-f702-654811c51bdb"
      },
      "source": [
        "def build_vocab(tokens):\n",
        "    tok2idx = collections.defaultdict(lambda: 0)\n",
        "    unique_tokens = list(functools.reduce(lambda a, b: set(a).union(b), tokens))\n",
        "    print(len(unique_tokens))\n",
        "    if len(unique_tokens) > 1000:\n",
        "        max_len = int(len(unique_tokens) * 1.)\n",
        "        wc = collections.Counter(functools.reduce(lambda a, b: a + b, tokens))\n",
        "        unique_tokens = [*map(lambda w: w[0], wc.most_common(max_len))]\n",
        "    print(f'vocab size: {len(unique_tokens)}')\n",
        "    idx2tok = ['<UNK>', '<START>', '<END>', '<PAD>'] + unique_tokens\n",
        "    for i, tok in enumerate(idx2tok):\n",
        "        tok2idx[tok] = i\n",
        "    return tok2idx, idx2tok\n",
        "\n",
        "tok2idx, idx2tok = build_vocab(tokens)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25670\n",
            "vocab size: 25670\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2feWQFyRIwG6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad_train(x, tok2idx, seq_len):\n",
        "    x = [tok2idx['<START>']] + x\n",
        "    if len(x) >= seq_len:\n",
        "        return np.array(x[:seq_len], dtype=np.int32), seq_len\n",
        "    pad_x = x + [tok2idx['<PAD>']] * (seq_len - len(x))\n",
        "    return np.array(pad_x, dtype=np.int32), len(x)\n",
        "\n",
        "\n",
        "def pad_target(y, tok2idx, seq_len):\n",
        "    y = y + [tok2idx['<END>']]\n",
        "    if len(y) >= seq_len:\n",
        "        return np.array(y[:seq_len], dtype=np.int32), seq_len\n",
        "    pad_y = y + [tok2idx['<PAD>']] * (seq_len - len(y))\n",
        "    return np.array(pad_y, dtype=np.int32), len(y)\n",
        "\n",
        "\n",
        "def batches_generator(batch_size, tokens, tok2idx, seq_len):\n",
        "    n_samples = len(tokens)\n",
        "    order = np.random.permutation(n_samples) # shuffle data\n",
        "    n_batches = n_samples // batch_size + 1\n",
        "    for k in range(n_batches):\n",
        "        batch_start = k * batch_size\n",
        "        batch_end = min((k + 1) * batch_size, n_samples)\n",
        "        current_batch_size = batch_end - batch_start\n",
        "        x_list, y_list, max_len = [], [], 0\n",
        "        for idx in order[batch_start:batch_end]:\n",
        "            x_list.append([tok2idx[t] for t in tokens[idx][:-1]])\n",
        "            y_list.append([tok2idx[t] for t in tokens[idx][1:]])\n",
        "            max_len = max(max_len, len(tokens[idx]))\n",
        "        X = np.ones([current_batch_size, max_len], dtype=np.int32) * tok2idx['<PAD>']\n",
        "        Y = np.ones([current_batch_size, max_len], dtype=np.int32) * tok2idx['<PAD>']\n",
        "        actual_lengths = np.zeros(current_batch_size, dtype=np.int32)\n",
        "        for n in range(current_batch_size):\n",
        "            tok_len = len(x_list[n])\n",
        "            actual_lengths[n] = tok_len\n",
        "            X[n, :tok_len] = x_list[n]\n",
        "            Y[n, :tok_len] = y_list[n]\n",
        "        yield X, Y, actual_lengths"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqVwUhrdktEA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x, y, lens = next(batches_generator(16, tokens, tok2idx, 0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iA-GrEeWIwG-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NeuralLanguageModel:\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEEpc9vIIwHC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def declare_placeholders(self):\n",
        "    self.input_batch = tf.placeholder(dtype=tf.int32, shape=[None, None], name='input_batch')\n",
        "    self.target_batch = tf.placeholder(dtype=tf.int32, shape=[None, None], name='target_batch')\n",
        "    self.lengths = tf.placeholder(dtype=tf.int32, shape=[None], name='lengths')\n",
        "    self.dropout_ph = tf.placeholder_with_default(tf.cast(1.0, tf.float32), shape=[], name='dropout_rate')\n",
        "    self.learning_rate_ph = tf.placeholder(dtype=tf.float32, shape=[], name='learning_rate')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_46V5ldwIwHM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NeuralLanguageModel.__declare_placeholders = classmethod(declare_placeholders)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dk8G3YYHIwHR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_layers(self, vocabulary_size, embedding_dim, n_hidden):\n",
        "    initial_embedding_matrix = np.random.randn(vocabulary_size, embedding_dim) / np.sqrt(embedding_dim)\n",
        "    embedding_matrix = tf.Variable(initial_embedding_matrix, dtype=tf.float32, name='embedding_matrix')\n",
        "    \n",
        "    rnn_cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)\n",
        "    # rnn_cell = tf.nn.rnn_cell.BasicRNNCell(num_units=n_hidden, activation=tf.nn.tanh)\n",
        "    regulrized_rnn_cell = tf.nn.rnn_cell.DropoutWrapper(\n",
        "        cell=rnn_cell, input_keep_prob=self.dropout_ph, output_keep_prob=self.dropout_ph,\n",
        "        state_keep_prob=self.dropout_ph)\n",
        "    \n",
        "    mul_cell = tf.nn.rnn_cell.MultiRNNCell(cells=[regulrized_rnn_cell]*2)\n",
        "    embeddings = tf.nn.embedding_lookup(embedding_matrix, self.input_batch)\n",
        "    output, state = tf.nn.dynamic_rnn(cell=mul_cell,\n",
        "                                      inputs=embeddings,\n",
        "                                      sequence_length=self.lengths,\n",
        "                                      dtype=tf.float32)\n",
        "    self.logits = tf.layers.dense(output, vocabulary_size, activation=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voLH2_eVIwHV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NeuralLanguageModel.__build_layers = classmethod(build_layers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihSDRFSiIwHb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_predictions(self):\n",
        "    softmax_output = tf.nn.softmax(self.logits)\n",
        "    self.predictions = tf.argmax(softmax_output, axis=-1)\n",
        "    self.probs = softmax_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjQU8mLqIwHg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NeuralLanguageModel.__compute_predictions = classmethod(compute_predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vj385LXXIwHl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_loss(self, vocabulary_size, pad_index):\n",
        "    with tf.device('/device:GPU:0'):\n",
        "        targets_one_hot = tf.one_hot(self.target_batch, vocabulary_size)\n",
        "        loss_tensor = tf.nn.softmax_cross_entropy_with_logits_v2(labels=targets_one_hot,\n",
        "                                                                logits=self.logits)\n",
        "        mask = tf.cast(tf.not_equal(self.input_batch, pad_index), tf.float32)\n",
        "        self.loss = tf.reduce_mean(tf.boolean_mask(loss_tensor, mask))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxXKY-UGIwHr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NeuralLanguageModel.__compute_loss = classmethod(compute_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IY0gppJtIwHw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def perform_optimization(self):\n",
        "    with tf.device('/device:GPU:0'):\n",
        "        self.optimizer = tf.train.AdamOptimizer(self.learning_rate_ph)\n",
        "        grads_and_vars = self.optimizer.compute_gradients(self.loss)\n",
        "        \n",
        "        clip_norm = tf.cast(5.0, tf.float32)\n",
        "        self.grads_and_vars = [(tf.clip_by_norm(grad, clip_norm), var) for grad, var in grads_and_vars]\n",
        "        \n",
        "        self.train_op = self.optimizer.apply_gradients(self.grads_and_vars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHttKRs3IwH2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NeuralLanguageModel.__perform_optimization = classmethod(perform_optimization)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnCjjK-tIwH6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init_model(self, vocabulary_size, embedding_dim, n_hidden, pad_index):\n",
        "    self.__declare_placeholders()\n",
        "    self.__build_layers(vocabulary_size, embedding_dim, n_hidden)\n",
        "    self.__compute_predictions()\n",
        "    self.__compute_loss(vocabulary_size, pad_index)\n",
        "    self.__perform_optimization()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xC25ldcHIwH_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NeuralLanguageModel.__init__ = classmethod(init_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nKNoZX-IwII",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_on_batch(self, sess, x_batch, y_batch, lengths, learning_rate, dropout_keep_prob):\n",
        "    feed_dict = {\n",
        "        self.input_batch: x_batch,\n",
        "        self.target_batch: y_batch,\n",
        "        self.learning_rate_ph: learning_rate,\n",
        "        self.dropout_ph: dropout_keep_prob,\n",
        "        self.lengths: lengths\n",
        "    }\n",
        "    sess.run(self.train_op, feed_dict=feed_dict)\n",
        "    return sess.run(self.loss, feed_dict=feed_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dp_F17nJIwIM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NeuralLanguageModel.train_on_batch = classmethod(train_on_batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIjQEk66IwIP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_for_batch(self, sess, x_batch, lengths):\n",
        "    feed_dict = {\n",
        "        self.input_batch: x_batch,\n",
        "        self.lengths: lengths\n",
        "    }\n",
        "    predictions = sess.run(self.predictions, feed_dict=feed_dict)\n",
        "    return predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZ_Dj4glIwIT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NeuralLanguageModel.predict_for_batch = classmethod(predict_for_batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGnQhjNWIwIY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "model = NeuralLanguageModel(vocabulary_size=len(tok2idx),\n",
        "                            embedding_dim=256,\n",
        "                            n_hidden=256,\n",
        "                            pad_index=tok2idx['<PAD>'])\n",
        "batch_size = 50\n",
        "n_epochs = 50\n",
        "learning_rate = 0.002\n",
        "learning_rate_decay = 1.03\n",
        "dropout_keep_probability = 1."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfe_Wnc5IwIc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "652d676f-ac06-4474-c2d1-cfe1005718d0"
      },
      "source": [
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "for epoch in range(n_epochs):\n",
        "    # For each epoch evaluate the model on train and validation data\n",
        "    print('-' * 20 + ' Epoch {} '.format(epoch+1) + 'of {} '.format(n_epochs) + '-' * 20)\n",
        "    \n",
        "    # Train the model\n",
        "    losses = []\n",
        "    for x_batch, y_batch, lengths in batches_generator(batch_size, tokens, tok2idx, 100):\n",
        "        batch_loss = model.train_on_batch(sess, x_batch, y_batch, lengths, learning_rate,\n",
        "                                          dropout_keep_probability)\n",
        "        losses.append(batch_loss * len(x_batch))\n",
        "    print(f'traning loss: {np.sum(losses)/len(tokens)}')\n",
        "        \n",
        "    # Decaying the learning rate\n",
        "    learning_rate = learning_rate / learning_rate_decay\n",
        "    \n",
        "print('...training finished.')"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------------- Epoch 1 of 50 --------------------\n",
            "traning loss: 8.198165399496284\n",
            "-------------------- Epoch 2 of 50 --------------------\n",
            "traning loss: 7.581930865062756\n",
            "-------------------- Epoch 3 of 50 --------------------\n",
            "traning loss: 7.137873792724855\n",
            "-------------------- Epoch 4 of 50 --------------------\n",
            "traning loss: 6.727495897639047\n",
            "-------------------- Epoch 5 of 50 --------------------\n",
            "traning loss: 6.400176626912663\n",
            "-------------------- Epoch 6 of 50 --------------------\n",
            "traning loss: 6.102600703461404\n",
            "-------------------- Epoch 7 of 50 --------------------\n",
            "traning loss: 5.82996601086367\n",
            "-------------------- Epoch 8 of 50 --------------------\n",
            "traning loss: 5.564170134010131\n",
            "-------------------- Epoch 9 of 50 --------------------\n",
            "traning loss: 5.309084876199787\n",
            "-------------------- Epoch 10 of 50 --------------------\n",
            "traning loss: 5.057772687311923\n",
            "-------------------- Epoch 11 of 50 --------------------\n",
            "traning loss: 4.8129616560560935\n",
            "-------------------- Epoch 12 of 50 --------------------\n",
            "traning loss: 4.5721728572110685\n",
            "-------------------- Epoch 13 of 50 --------------------\n",
            "traning loss: 4.341785182922266\n",
            "-------------------- Epoch 14 of 50 --------------------\n",
            "traning loss: 4.115864295446662\n",
            "-------------------- Epoch 15 of 50 --------------------\n",
            "traning loss: 3.8992308135208695\n",
            "-------------------- Epoch 16 of 50 --------------------\n",
            "traning loss: 3.689137013727742\n",
            "-------------------- Epoch 17 of 50 --------------------\n",
            "traning loss: 3.4967147274537798\n",
            "-------------------- Epoch 18 of 50 --------------------\n",
            "traning loss: 3.3134981359371594\n",
            "-------------------- Epoch 19 of 50 --------------------\n",
            "traning loss: 3.145074168330785\n",
            "-------------------- Epoch 20 of 50 --------------------\n",
            "traning loss: 2.9860338349594926\n",
            "-------------------- Epoch 21 of 50 --------------------\n",
            "traning loss: 2.840643524740902\n",
            "-------------------- Epoch 22 of 50 --------------------\n",
            "traning loss: 2.705432791579784\n",
            "-------------------- Epoch 23 of 50 --------------------\n",
            "traning loss: 2.578497081660153\n",
            "-------------------- Epoch 24 of 50 --------------------\n",
            "traning loss: 2.4596741151656616\n",
            "-------------------- Epoch 25 of 50 --------------------\n",
            "traning loss: 2.3501535202488664\n",
            "-------------------- Epoch 26 of 50 --------------------\n",
            "traning loss: 2.2483550687088822\n",
            "-------------------- Epoch 27 of 50 --------------------\n",
            "traning loss: 2.1527817394721946\n",
            "-------------------- Epoch 28 of 50 --------------------\n",
            "traning loss: 2.0631981657557845\n",
            "-------------------- Epoch 29 of 50 --------------------\n",
            "traning loss: 1.9769791461300124\n",
            "-------------------- Epoch 30 of 50 --------------------\n",
            "traning loss: 1.8994616886203208\n",
            "-------------------- Epoch 31 of 50 --------------------\n",
            "traning loss: 1.8263101669605433\n",
            "-------------------- Epoch 32 of 50 --------------------\n",
            "traning loss: 1.7545030899644856\n",
            "-------------------- Epoch 33 of 50 --------------------\n",
            "traning loss: 1.6905926418151365\n",
            "-------------------- Epoch 34 of 50 --------------------\n",
            "traning loss: 1.628704352803636\n",
            "-------------------- Epoch 35 of 50 --------------------\n",
            "traning loss: 1.5701908844240595\n",
            "-------------------- Epoch 36 of 50 --------------------\n",
            "traning loss: 1.5129952552230552\n",
            "-------------------- Epoch 37 of 50 --------------------\n",
            "traning loss: 1.465091668392261\n",
            "-------------------- Epoch 38 of 50 --------------------\n",
            "traning loss: 1.4137212811082744\n",
            "-------------------- Epoch 39 of 50 --------------------\n",
            "traning loss: 1.3678184663503166\n",
            "-------------------- Epoch 40 of 50 --------------------\n",
            "traning loss: 1.3242194357882724\n",
            "-------------------- Epoch 41 of 50 --------------------\n",
            "traning loss: 1.2836964520749847\n",
            "-------------------- Epoch 42 of 50 --------------------\n",
            "traning loss: 1.2438317765967613\n",
            "-------------------- Epoch 43 of 50 --------------------\n",
            "traning loss: 1.20637508973264\n",
            "-------------------- Epoch 44 of 50 --------------------\n",
            "traning loss: 1.1730852269150471\n",
            "-------------------- Epoch 45 of 50 --------------------\n",
            "traning loss: 1.1390232187882663\n",
            "-------------------- Epoch 46 of 50 --------------------\n",
            "traning loss: 1.1069617935398035\n",
            "-------------------- Epoch 47 of 50 --------------------\n",
            "traning loss: 1.0760008439684756\n",
            "-------------------- Epoch 48 of 50 --------------------\n",
            "traning loss: 1.0485717457140622\n",
            "-------------------- Epoch 49 of 50 --------------------\n",
            "traning loss: 1.0214380094270261\n",
            "-------------------- Epoch 50 of 50 --------------------\n",
            "traning loss: 0.996229486136337\n",
            "...training finished.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byoUqC7-Njrn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "d098f52b-bbc6-435a-9c5b-27e142a467a7"
      },
      "source": [
        "input_seq = tokens[18]\n",
        "print(input_seq)\n",
        "x = [[tok2idx['<START>']] + [*map(tok2idx.get, input_seq)]]\n",
        "feed_dict = {model.input_batch: x, model.lengths: [*map(len, x)]}\n",
        "probs = sess.run(model.probs, feed_dict=feed_dict)\n",
        "print([idx2tok[i] for i in probs[0].argmax(axis=1)])"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['First', 'Citizen:', 'I', 'say', 'unto', 'you,', 'what', 'he', 'hath', 'done', 'famously,', 'he', 'did', 'it', 'to', 'that', 'end:', 'though', 'soft-conscienced', 'men', 'can', 'be', 'content', 'to', 'say', 'it', 'was', 'for', 'his', 'country', 'he', 'did', 'it', 'to', 'please', 'his', 'mother', 'and', 'to', 'be', 'partly', 'proud;', 'which', 'he', 'is,', 'even', 'till', 'the', 'altitude', 'of', 'his', 'virtue.']\n",
            "['I', 'Citizen:', 'I', 'have', 'it', 'your', 'and', 'he', 'should', 'done', 'famously,', 'he', 'did', 'it', 'to', 'the', 'end:', 'though', 'soft-conscienced', 'men', 'can', 'be', 'content', 'to', 'say', 'it', 'is', 'an', 'his', 'country', 'he', 'did', 'lent', 'to', 'use', 'his', 'mother', 'and', 'to', 'have', 'of', 'proud;', 'which', 'he', 'is,', 'even', 'so', 'the', 'altitude', 'of', 'his', 'virtue.', 'By']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ra0qUBx5Oqs4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "dfed39e2-1ac2-4935-95ec-38479420e709"
      },
      "source": [
        "de = nltk.tokenize.treebank.TreebankWordDetokenizer()\n",
        "de.detokenize([idx2tok[i] for i in probs[0].argmax(axis=1)])"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I Citizen: I have it your and he should done famously, he did it to the end: though soft-conscienced men can be content to say it is an his country he did lent to use his mother and to have of proud; which he is, even so the altitude of his virtue. By'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2aTkAJFOLRI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vE246Fs7OLND",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vpk06tJ3IwIg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def get_cond_prob(sess, model, seq_toks):\n",
        "#     feed_dict = {\n",
        "#         model.input_batch: seq_toks,\n",
        "#         model.lengths: [len(seq_toks)+10] * len(seq_toks)\n",
        "#     }\n",
        "#     return np.log1p(sess.run(model.probs, feed_dict=feed_dict)[:, -1])\n",
        "\n",
        "\n",
        "# def find_topk_2d(arr, topk):\n",
        "# #     arr[:, 2] = 0\n",
        "# #     arr[:, 6] = 0\n",
        "# #     arr[:, 5] = 0\n",
        "# #     arr[:, 7] = 0\n",
        "# #     arr[:, 11] = 0\n",
        "# #     arr[:, 4] = 0\n",
        "#     order = arr.reshape(-1,).argsort()[::-1][:topk]\n",
        "#     idx = np.unravel_index(order, arr.shape)\n",
        "#     vals = []\n",
        "#     for i in range(topk):\n",
        "#         vals.append(arr[idx[0][i], idx[1][i]])\n",
        "#     return vals, idx\n",
        "\n",
        "\n",
        "# def init_beam_search(sess, model, start_toks, topk):\n",
        "#     cond_probs = get_cond_prob(sess, model, start_toks)\n",
        "#     vals, idx = find_topk_2d(cond_probs, topk)\n",
        "#     seq_toks = np.column_stack((np.repeat(start_toks, topk, axis=0),\n",
        "#                                 idx[1].reshape(-1,1)))\n",
        "#     seq_probs = np.reshape(vals, (-1, 1))\n",
        "#     return seq_toks, seq_probs\n",
        "\n",
        "\n",
        "# def extend_seq(seq_toks, idx):\n",
        "#     new_seq = []\n",
        "#     for i in range(len(seq_toks)):\n",
        "#         seq_idx = idx[0][i]\n",
        "#         tok_idx = idx[1][i]\n",
        "#         new_seq.append(np.array(list(seq_toks[seq_idx]) + [tok_idx]))\n",
        "#     return np.array(new_seq)\n",
        "\n",
        "\n",
        "# def calculate_seq_probs(seq_probs, cond_probs):\n",
        "#     return cond_probs + seq_probs.reshape(-1, 1)\n",
        "    \n",
        "\n",
        "# def itrate_search(sess, seq_toks, seq_probs, topk):\n",
        "#     cond_probs = get_cond_prob(sess, model, seq_toks)\n",
        "#     seq_probs = calculate_seq_probs(seq_probs, cond_probs)\n",
        "#     vals, idx = find_topk_2d(seq_probs, topk)\n",
        "#     seq_toks = extend_seq(seq_toks, idx)\n",
        "#     seq_probs = seq_probs[idx[0], idx[1]]\n",
        "#     return seq_toks, seq_probs\n",
        "\n",
        "# def beam_search(sess, model, start_toks, topk, idx2tok):\n",
        "#     detokenizer = nltk.tokenize.treebank.TreebankWordDetokenizer()\n",
        "#     seq_toks, seq_probs = init_beam_search(sess, model, start_toks, topk)\n",
        "#     for i in range(10):\n",
        "#         seq_toks, seq_probs = itrate_search(sess, seq_toks, seq_probs, topk)\n",
        "#     output_sents = []\n",
        "#     for seq in seq_toks:\n",
        "#         sent = detokenizer.detokenize([idx2tok[i] for i in seq])\n",
        "#         output_sents.append(sent)\n",
        "#     return output_sents, seq_toks, seq_probs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATly7ZzEIwJz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3dPg00_IwJ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}